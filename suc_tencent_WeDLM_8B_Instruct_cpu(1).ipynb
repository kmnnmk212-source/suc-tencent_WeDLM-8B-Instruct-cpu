{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evhIjBbIGpOu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "piwU5oi-Gzsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wZOrXkxbGzvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BzrKuoJgGzx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install psutil ninja packaging"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDTaRgqpHUxp",
        "outputId": "6a7b212e-20bd-4bbd-e334-219d812d81be"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (5.9.5)\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (25.0)\n",
            "Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r 1.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UHSV9pRaHvNb",
        "outputId": "639071bd-6a9e-49cf-e139-4691f4f7051b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.56.1 (from -r 1.txt (line 2))\n",
            "  Downloading transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/42.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.4.0 (from -r 1.txt (line 3))\n",
            "  Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: xxhash==3.6.0 in /usr/local/lib/python3.12/dist-packages (from -r 1.txt (line 4)) (3.6.0)\n",
            "Collecting numpy==2.3.4 (from -r 1.txt (line 5))\n",
            "  Downloading numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (from -r 1.txt (line 6)) (4.67.1)\n",
            "Collecting safetensors==0.6.2 (from -r 1.txt (line 7))\n",
            "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting huggingface_hub==0.35.0 (from -r 1.txt (line 8))\n",
            "  Downloading huggingface_hub-0.35.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting flask==3.1.1 (from -r 1.txt (line 9))\n",
            "  Downloading flask-3.1.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from -r 1.txt (line 12)) (5.9.5)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from -r 1.txt (line 13)) (1.13.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from -r 1.txt (line 14)) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.1->-r 1.txt (line 2)) (3.20.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.1->-r 1.txt (line 2)) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.1->-r 1.txt (line 2)) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.1->-r 1.txt (line 2)) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.1->-r 1.txt (line 2)) (0.22.1)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.12/dist-packages (from triton==3.4.0->-r 1.txt (line 3)) (75.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub==0.35.0->-r 1.txt (line 8)) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub==0.35.0->-r 1.txt (line 8)) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub==0.35.0->-r 1.txt (line 8)) (1.2.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask==3.1.1->-r 1.txt (line 9)) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask==3.1.1->-r 1.txt (line 9)) (8.3.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask==3.1.1->-r 1.txt (line 9)) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from flask==3.1.1->-r 1.txt (line 9)) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask==3.1.1->-r 1.txt (line 9)) (3.0.3)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from flask==3.1.1->-r 1.txt (line 9)) (3.1.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.1->-r 1.txt (line 2)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.1->-r 1.txt (line 2)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.1->-r 1.txt (line 2)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.1->-r 1.txt (line 2)) (2025.11.12)\n",
            "Downloading transformers-4.56.1-py3-none-any.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m485.8/485.8 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.35.0-py3-none-any.whl (563 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m563.4/563.4 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flask-3.1.1-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, safetensors, numpy, huggingface_hub, flask, transformers\n",
            "  Attempting uninstall: safetensors\n",
            "    Found existing installation: safetensors 0.7.0\n",
            "    Uninstalling safetensors-0.7.0:\n",
            "      Successfully uninstalled safetensors-0.7.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.36.0\n",
            "    Uninstalling huggingface-hub-0.36.0:\n",
            "      Successfully uninstalled huggingface-hub-0.36.0\n",
            "  Attempting uninstall: flask\n",
            "    Found existing installation: Flask 3.1.2\n",
            "    Uninstalling Flask-3.1.2:\n",
            "      Successfully uninstalled Flask-3.1.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.3\n",
            "    Uninstalling transformers-4.57.3:\n",
            "      Successfully uninstalled transformers-4.57.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed flask-3.1.1 huggingface_hub-0.35.0 numpy-2.3.4 safetensors-0.6.2 transformers-4.56.1 triton-3.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "safetensors",
                  "transformers"
                ]
              },
              "id": "b4ecb9931b81492f982e9f91e9eb49d2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT use: pip install -r requirements.txt\n",
        "# Use: bash install.sh\n",
        "#\n",
        "# This file pins versions for reference only.\n",
        "# flash-attn requires torch first and --no-build-isolation flag.\n",
        "\n",
        "--extra-index-url https://download.pytorch.org/whl/cu129\n",
        "torch==2.8.0+cu129\n",
        "transformers==4.56.1\n",
        "triton==3.4.0\n",
        "xxhash==3.6.0\n",
        "numpy==2.3.4\n",
        "tqdm==4.67.1\n",
        "safetensors==0.6.2\n",
        "huggingface_hub==0.35.0\n",
        "flask==3.1.1\n",
        "\n",
        "# flash-attn build dependencies\n",
        "psutil\n",
        "ninja\n",
        "packaging\n",
        "\n",
        "# flash-attn (install with: pip install flash-attn --no-build-isolation)\n",
        "flash-attn==2.7.4.post1"
      ],
      "metadata": {
        "id": "7iZhrvvvHiS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Tencent/WeDLM.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JETRXnIHIAUb",
        "outputId": "39481102-20a8-4b81-cc79-f8ca8e05930d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'WeDLM'...\n",
            "remote: Enumerating objects: 113, done.\u001b[K\n",
            "remote: Counting objects: 100% (113/113), done.\u001b[K\n",
            "remote: Compressing objects: 100% (86/86), done.\u001b[K\n",
            "remote: Total 113 (delta 33), reused 102 (delta 25), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (113/113), 6.46 MiB | 6.20 MiB/s, done.\n",
            "Resolving deltas: 100% (33/33), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/WeDLM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RqoM3roIHst",
        "outputId": "baed5e3b-3e6b-4eb5-8a83-9447cce37e64"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/WeDLM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from wedlm import LLM, SamplingParams\n",
        "\n",
        "llm = LLM(model=\"tencent/WeDLM-8B-Instruct\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"tencent/WeDLM-8B-Instruct\", trust_remote_code=True, torch_dtype=\"auto\", device_map=\"auto\")\n",
        "\n",
        "prompt = \"Solve step by step: A store sells apples for $2 each and oranges for $3 each. Tom bought 5 apples and 4 oranges. How much did he spend?\"\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "outputs = llm.generate([text], SamplingParams(temperature=0.2, max_tokens=512))\n",
        "print(outputs[0][\"text\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "RbPir1LiGzzj",
        "outputId": "0b55b712-9196-4e08-aac6-b770b7116956"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'flash_attn'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1035661451.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwedlm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSamplingParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tencent/WeDLM-8B-Instruct\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tencent/WeDLM-8B-Instruct\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/WeDLM/wedlm/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwedlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwedlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampling_params\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSamplingParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/WeDLM/wedlm/llm.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwedlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_engine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLLMEngine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/WeDLM/wedlm/engine/llm_engine.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwedlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwedlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mScheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwedlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_runner\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelRunner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/WeDLM/wedlm/engine/model_runner.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwedlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwedlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwedlm_decoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWeDLMDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwedlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwedlm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWeDLMForDiffusionLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwedlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwedlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/WeDLM/wedlm/models/wedlm.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwedlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSiluAndMul\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwedlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwedlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRMSNorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m from wedlm.layers.linear import (\n",
            "\u001b[0;32m/content/WeDLM/wedlm/layers/attention.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mflash_attn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mflash_attn_varlen_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwedlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flash_attn'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7b8884c",
        "outputId": "3c229739-4dda-4ba5-f686-b9b1b449c7fd"
      },
      "source": [
        "import os\n",
        "if not os.path.exists('install.sh'):\n",
        "  print('install.sh not found. Please ensure you are in the /content/WeDLM directory.')\n",
        "else:\n",
        "  !bash install.sh"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== WeDLM Installation ===\n",
            "CUDA: cu129 | PyTorch: 2.8.0 | flash-attn: 2.7.4.post1\n",
            "\n",
            "[1/4] Installing PyTorch...\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu129\n",
            "Collecting torch==2.8.0+cu129\n",
            "  Using cached https://download.pytorch.org/whl/cu129/torch-2.8.0%2Bcu129-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu129) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu129) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu129) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu129) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu129) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu129) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu129) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.9.86 (from torch==2.8.0+cu129)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cuda-nvrtc-cu12/nvidia_cuda_nvrtc_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (89.6 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m89.6/89.6 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.9.79 (from torch==2.8.0+cu129)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cuda-runtime-cu12/nvidia_cuda_runtime_cu12-12.9.79-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.9.79 (from torch==2.8.0+cu129)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cuda-cupti-cu12/nvidia_cuda_cupti_cu12-12.9.79-py3-none-manylinux_2_25_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.10.2.21 (from torch==2.8.0+cu129)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cudnn-cu12/nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "\u001b[2K     \u001b[91m\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m64.9/706.8 MB\u001b[0m \u001b[31m113.2 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[91m\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m66.9/706.8 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:06\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18eb33dd",
        "outputId": "627015f7-6f8d-4c36-f483-a8352e101968"
      },
      "source": [
        "import os\n",
        "if not os.path.exists('install.sh'):\n",
        "  print('install.sh not found. Please ensure you are in the /content/WeDLM directory.')\n",
        "else:\n",
        "  !bash install.sh"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== WeDLM Installation ===\n",
            "CUDA: cu129 | PyTorch: 2.8.0 | flash-attn: 2.7.4.post1\n",
            "\n",
            "[1/4] Installing PyTorch...\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu129\n",
            "Collecting torch==2.8.0+cu129\n",
            "  Downloading https://download.pytorch.org/whl/cu129/torch-2.8.0%2Bcu129-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu129) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu129) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu129) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu129) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu129) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu129) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu129) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.9.86 (from torch==2.8.0+cu129)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cuda-nvrtc-cu12/nvidia_cuda_nvrtc_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (89.6 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m89.6/89.6 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.9.79 (from torch==2.8.0+cu129)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cuda-runtime-cu12/nvidia_cuda_runtime_cu12-12.9.79-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m131.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.9.79 (from torch==2.8.0+cu129)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cuda-cupti-cu12/nvidia_cuda_cupti_cu12-12.9.79-py3-none-manylinux_2_25_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.10.2.21 (from torch==2.8.0+cu129)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cudnn-cu12/nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.9.1.4 (from torch==2.8.0+cu129)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cublas-cu12/nvidia_cublas_cu12-12.9.1.4-py3-none-manylinux_2_27_x86_64.whl (581.2 MB)\n",
            "\u001b[2K     \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m204.9/581.2 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:01:39\u001b[0m\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m205.0/581.2 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:01:40\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is the derivative of x^2?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"The derivative of x\u00b2 is 2x.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What about x^3?\"}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "outputs = llm.generate([text], SamplingParams(temperature=0.2, max_tokens=256))\n"
      ],
      "metadata": {
        "id": "CAYhjz0ZG4v0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"tencent/WeDLM-8B-Instruct\", trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"tencent/WeDLM-8B-Instruct\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
        "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "outputs = model(**inputs)\n"
      ],
      "metadata": {
        "id": "3lvagrLBLKB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"tencent/WeDLM-8B-Instruct\", trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"tencent/WeDLM-8B-Instruct\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
        "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "outputs = model(**inputs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756,
          "referenced_widgets": [
            "0eb26275b5ae4512a2dca94e84d64f7b",
            "b11c9dfb50aa4880b33e05b541079935",
            "b47f08002c09499092af3e73b7cccd5a",
            "ba495d9f82d444739d87dfb628ec3dc1",
            "c94f89dac65849338b2ac3f0a003c975",
            "aeeb1ffbbd5f4abbbbc7711b3afa916d",
            "7c32ee6bb7c24d3c81390d45124753c9",
            "dc4d775843dc44a0ac53d156d08145de",
            "3c742db0cf604e269710e0e85f970dd9",
            "21cb83e6745542f796eecaa77dc65478",
            "69778bc424ef4b9d896b7fb00fc95164",
            "562c3524333849dd81b24d832c9a5f2f",
            "6f77d48a620e4e13ba4506f37a5c3013",
            "8e49b0e7de4f4733918642213e52c986",
            "7aaf917b7e28454995f93e9597d953f8",
            "c0b68c73197c4a7d8b8f4bb03353de53",
            "1edeb23b84a44dea927a69d2bd267c4c",
            "b61a18e043c243bc97ed10e301962e7a",
            "dfdb136956d143e7b3576f983b30310f",
            "23ffc711899c462d85ffb0c420da3ba3",
            "bc629caab8d748b895cbf89a53db0c5b",
            "21c5c1ca9aac4d6fba0ced4105fbd2a7",
            "76ddf8399c1046a590d7407acb03ac49",
            "9740809f19c341c2ae9c127854cb4d47",
            "98ff53ad380c49ab9ef80eddb810a035",
            "eae5e47c226448e393f4fdda0c03d8d5",
            "90a73f43c6294570bc98330ba68c41c5",
            "d1071958849e483db0df4d22219bc615",
            "533d5c95f2494fb185d6eb27d0cff6c0",
            "20aff7c42618419493d779a0a75e77d0",
            "857b9398e61d42658928e57ffd60ee1f",
            "f238dd79cf374f4e84a8635a652fdcb8",
            "3565e71282694535aa0185e8880e93e5",
            "ed0c896778f2459e9e73044f78e92576",
            "fce984cdb24946249e539930f543ff86",
            "c648c9b97b8541d585a6e49c46592aac",
            "8eba8d38c0694b9397858b044aa5532b",
            "2ce35d13da52424ebe8278f7a288034b",
            "366e0b5692ba49cfb97ca6ac1781d4dd",
            "137d6b9e607a492fb81268b767421813",
            "3375ccb03c464059b9dfd5ea656278ed",
            "310942c524574743a011dcb71d8d3b34",
            "486bdad6557f4ee5adaaca1d270cb898",
            "c1a6ce6c661d41baa31f39acd3070249",
            "83e94ce64b294d9faddced5b52fd0e80",
            "1be21c3cfdfb47109b2d699c71876593",
            "34c29feda4dd4a4694740e4734282692",
            "6a82e908d2a64bc6b98fd056da215261",
            "1a3c985bef6546a7ae364e5322446a7b",
            "8e454ad204924ac79969c70a77477bf6",
            "b26847d086d6408eb5b18f73185fd9e0",
            "33706ff306ef4ff1bb383cc30f1a23e1",
            "57b170d8ee954df0ac6b84ddbb5379e9",
            "d3da6ec6ab434c52958718fdac1785ec",
            "155c8ae13cfd441eb2f52d6b6a3e6f65",
            "22e49e484f314443a1c6a3c294659dd5",
            "493fbbac82b04f1a9435f931af550776",
            "c823246e9fb34d75abfd7ca334174783",
            "04a798532a074c8c86a012883dab216b",
            "e5fedab294bb499886b9d50e8e3e4dd8",
            "04aec827ad6a4db3a817255a20fb1505",
            "62a9cbab18064a979ef291007e6c1b79",
            "1c69cff78ec74d2b92d9696378a35fb5",
            "eea80b90c12144f699c80c0e83e03efe",
            "610f6919f96c46d2804e8803c6ea5d4a",
            "31092ae8692543ab9dfdad1c1730da6a",
            "8981e539787b41acae6a300288d5f98a",
            "aa0704131cc5422aa5c1f22c7b38925e",
            "63ce0c847f504fddb1309d10b9d9026a",
            "7b4422a9ab4a4e3fa213840b4eb3ea57",
            "038c9104a1d748f6a234db52899417f3",
            "b2065d3bb40d4265aa7e44c59ea475fb",
            "b58a33d1e4434a06ac6031a375c145f7",
            "2a70cfa99b6b4fc88e549d3205eeb80c",
            "b026ec25bcba4db2a442b94e20c09ad1",
            "a6da40496f804b81a2c808b7e7a02805",
            "17cb3aa283214eaa9d2f5bc6bb2261d3",
            "9a1c29941120409789d95f103cca4250",
            "5b6a623bd47843d9b5c5bbf550c42dba",
            "4b2657f405ba44478beff494618b5c91",
            "cfb258d472284384ab39662378c32d96",
            "ed478b6a1e114a50971e83b75745cc03",
            "7993c10cdca44cbfbeb3d214106817f2",
            "c3d815fb94674facb90305fe988982ed",
            "1378a8182ffd4cd8bb9e8f67819a9718",
            "9ba35d85c5b44b9da5d4bc564a79eb91",
            "e20281237c34499fab5df77956a633d4",
            "4a2b777a4fb3444a9f81855f5ee50f42",
            "1358c7cf96134db793d4ac82718fd25e",
            "a913a1614c484a5f8a4939a5a556d480",
            "a0e3cbf6e19d477c8030b03d086f2b9b",
            "8b60b80b28d64d09bc3f5586da1e7052",
            "da5bdd9ca8374c0da9416cd3dd945d54",
            "5dba63103e3e4a53a0f4cb49966266fd",
            "8dddd1ca96cf4ca7bd3679f0197a8ba7",
            "929d92e27d0a45f1a042d6cd603817f4",
            "e9bf3e01a1d943aaa0e948cb690b2526",
            "621b030b97a543e8b484e770f5f6c014",
            "a0ed8e9b740145859b4902c28ada4ec6",
            "7b79392f6d5545d1826972fc505ddab4",
            "414e121b600f4de1a489e5746c96afe9",
            "0d72c615d3d04d118ee4eab73f406a69",
            "dbdfbd438c164327bbb3a617127d0e17",
            "14d011dc87d0493fbcdfcfba7c2c3d2d",
            "fe23022804804e5e97a6451a6eba542f",
            "274928b9384e4b82a6b21ca6f8aab80c",
            "6a37541dd3574d3ea1ffc0d3c8a371c4",
            "70baf326ffc6444e8868e450252016a5",
            "ed553318095444d4bfbefc80ba3a77eb",
            "c444d73a7f88465b8a0f4e2a9fd6e26f",
            "a1cec16ff45c42d190baeb40fbeca2da",
            "728dddabe9c644e5bef031ee301299bc",
            "944c0da94d914037b6db3819a3ffa4a3",
            "9214971633f84b4998d8b8a26ab636b3",
            "da3a3bc2b76240a094d58b648de9a44d",
            "22dc8f065dd44138bd6ad059ab2101de",
            "3a6f6aa9c63e4bf2afd6e8a3585b5adb",
            "0397da21681548d3945c6f70ba61d48f",
            "5919d21f9dd54ba29c14494e8ef02a6f",
            "5ba100cc16d849d39f8ed0796b964c88",
            "eb553e71566e481097782463cae2a216",
            "e7b900a77fe34aa79230b0cd57e69fcd",
            "d085818dc6d144b391b43c948f4d2e31",
            "38fdab0c1dc946259c48229cf376560b",
            "024151f675c648fb8932371fb20f0eea",
            "bdc1ae0435e9471d8c496605b36008ee",
            "0e2f332fd73d418f93be134592c1b82b",
            "21389d2834ac411caa7bf20cbec0a862",
            "2a8eec1000e14596bfec11ceaf372227",
            "c356021c876c40a49fac8ce3eceee501",
            "30e727dc5af24190b581f27083265237",
            "fffef4d7e20d46f481e4d2264d465f6d",
            "af01b076cf7f4381983bc81a9d6793e7",
            "94ca35cc069542e9906f9c700d8685b3",
            "0d6c15ad5ca444b888427cd0b27bbfd5",
            "e88d2474a51a41f9b15c14973f271931",
            "6e4eaa411336495b80036b7f605e0601",
            "0a6b8d0ed83c44b1a03eaa1bec4905dd",
            "5ea8b67825764c19a54cd6eeab875b93",
            "62ffe42051ef48d79ef03a8bb46a0a0a",
            "05b2ae920299449c99865f7b533d7b36",
            "ad6d9b7bb8ad4539b82b97b6b905a8b1",
            "16c32cde2c874caea557c94c889bace5",
            "77e06ddbf53a42cea895b7a6bfb964c4",
            "e751243332c94627afa8d6691ac35d53",
            "05fec8e203374e53b4327a0ccb8bfe49",
            "a433ce9144f74ff08c2d106263ed1d56",
            "0cde72bd90484856b5500f75be029eef",
            "37d296d125bb4d1ab7792aead3d74902",
            "500427cabea64033bbc07e841c6c86fc",
            "8a22bc9b6a554e0882834a85990cd042",
            "777353b5db354c9091faf124b9ed3e16",
            "909cba9e49094531a191731efcecf596",
            "b9dfddb5f18e4688b94578e4db7cc758",
            "0e15a891a9dc41838ada88a96ec5388a",
            "08b7be3b84c647f8b8b56a8d21fb19d7",
            "f8f61f3aed4c4b53bef7c940c47929b5",
            "c8fce8442d5b4e14b4418287e4c2ca00",
            "dc8101b7f06449aeb7d0ae755f4856be",
            "36a83a12d3054a8387d8f9b52bacf17f",
            "794017a1063b45a8a47485b2d94b699f",
            "4cbf2db3eff540a386bc871c595ca72d",
            "3652cd9c158042609961deebb0b2dc30",
            "efd2e2ee7a33491387f07e648308187b",
            "360d2b9620de49529ef2687a1823eef0",
            "7a7a3e2f930441f4a398914149c59b87",
            "0eae217f9ed84a4288474f0dd01632cb",
            "9c09c34fb6b44d69a7298779ea90dd7d",
            "e1fd4fcd41d44941b1904942f1f7079a",
            "dc10bcd2e3a34db7a407d09932490654",
            "1c1cb0fcbcb545a2a349385de037cb43",
            "cfb10a0f4af54d4d86d2133b7c873291",
            "fbb752f1d6ad4e06af9bfd8014c32b08",
            "5b72873e28c24062980c9defb6d8e3a6",
            "fc0d21c80eee4767b3ea68cb87530ca5",
            "c508a63d01c94e2a82d2f9717a7bc6b4",
            "00a0356d755d4a4497c680002b6d953c",
            "21102395853842d6a2efdfe4e2422931",
            "22a50fbb256448a1b8be5c5209930cfc",
            "d3bfb119aba04af7847bc06e5354f6c1",
            "e8d06fc179d34bfd9abadcab3f39b761",
            "74b8db1365e34517b7331d595b51676c",
            "7053cb228f094e0fbd8b84cdf5507f0f",
            "7d57db65bd0c4bdb81f9a7ce5580a362",
            "30e138c3cdf74198870a720dd4e8aab1",
            "a3b34bcd58cc4f45a90ed436fdb0f5c3",
            "6d5670f5e5d043f39d3183e845f5dcd4",
            "f497fa67187d4c758a4b7273a7f16b48",
            "fc43a880d5824342bba99f30e3285bfe",
            "a3a728817be34df5920df296145a0ef8",
            "9bece009fe7942cfa78e2071fce00d8f",
            "9ef9c6113eb54e14a8c085d7e233eb11",
            "b85e2acab35644c18f51849ed8afed41",
            "2276c87631a84810ab7e6a8ee3198f4d",
            "351117efd4ac4a6f857b3c5648093e6f",
            "422b41f7fe4c40148a3a60faeb393dd3",
            "4908f6bd25b7437f8bc4661d9011bf44",
            "5756ad29d60e4ca3aed3d455811b5101"
          ]
        },
        "id": "KyzZhW5zG9TE",
        "outputId": "17d867d2-95a5-463e-edbf-7d7574e6af5a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0eb26275b5ae4512a2dca94e84d64f7b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "562c3524333849dd81b24d832c9a5f2f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76ddf8399c1046a590d7407acb03ac49"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed0c896778f2459e9e73044f78e92576"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83e94ce64b294d9faddced5b52fd0e80"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22e49e484f314443a1c6a3c294659dd5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "chat_template.jinja: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8981e539787b41acae6a300288d5f98a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a1c29941120409789d95f103cca4250"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "configuration_wedlm.py: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1358c7cf96134db793d4ac82718fd25e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/tencent/WeDLM-8B-Instruct:\n",
            "- configuration_wedlm.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modeling_wedlm.py: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b79392f6d5545d1826972fc505ddab4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/tencent/WeDLM-8B-Instruct:\n",
            "- modeling_wedlm.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1cec16ff45c42d190baeb40fbeca2da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7b900a77fe34aa79230b0cd57e69fcd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af01b076cf7f4381983bc81a9d6793e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77e06ddbf53a42cea895b7a6bfb964c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e15a891a9dc41838ada88a96ec5388a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/1.58G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a7a3e2f930441f4a398914149c59b87"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00a0356d755d4a4497c680002b6d953c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/146 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f497fa67187d4c758a4b7273a7f16b48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8b4m6N0K_om",
        "outputId": "4d9da4ba-1c14-4b4b-d9e2-a7b630fd2edb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CausalLMOutputWithPast(loss={'logits': tensor([[[21.7500, 23.8750, 21.3750,  ...,  9.6250,  9.6250,  9.6250],\n",
            "         [ 9.2500, 14.8750,  7.3438,  ..., -1.0312, -1.0312, -1.0312],\n",
            "         [15.0000, 16.5000, 18.3750,  ...,  4.3438,  4.3438,  4.3438],\n",
            "         ...,\n",
            "         [18.2500, 11.5625, 13.8750,  ...,  4.7500,  4.7500,  4.7500],\n",
            "         [ 8.5625,  3.9219,  6.2188,  ...,  5.3438,  5.3438,  5.3438],\n",
            "         [13.8125, 15.1250, 12.9375,  ...,  1.4219,  1.4219,  1.4219]]],\n",
            "       dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>), 'past_key_values': DynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer])}, logits=tensor([[[21.7500, 23.8750, 21.3750,  ...,  9.6250,  9.6250,  9.6250],\n",
            "         [ 9.2500, 14.8750,  7.3438,  ..., -1.0312, -1.0312, -1.0312],\n",
            "         [15.0000, 16.5000, 18.3750,  ...,  4.3438,  4.3438,  4.3438],\n",
            "         ...,\n",
            "         [18.2500, 11.5625, 13.8750,  ...,  4.7500,  4.7500,  4.7500],\n",
            "         [ 8.5625,  3.9219,  6.2188,  ...,  5.3438,  5.3438,  5.3438],\n",
            "         [13.8125, 15.1250, 12.9375,  ...,  1.4219,  1.4219,  1.4219]]],\n",
            "       dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>), past_key_values=DynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer]), hidden_states=None, attentions=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f413280",
        "outputId": "7889d1db-5e0e-40af-9561-cfda4af5700a"
      },
      "source": [
        "# \u0644\u062a\u0648\u0644\u064a\u062f \u0631\u062f \u0641\u0639\u0644\u064a \u0645\u0646 \u0627\u0644\u0646\u0645\u0648\u0630\u062c\n",
        "output_tokens = model.generate(**inputs, max_new_tokens=50, do_sample=True, top_k=50, top_p=0.95, temperature=0.7)\n",
        "\n",
        "# \u0641\u0643 \u062a\u0631\u0645\u064a\u0632 \u0627\u0644\u0631\u0645\u0648\u0632 \u0627\u0644\u0646\u0627\u062a\u062c\u0629 \u0625\u0644\u0649 \u0646\u0635 \u0642\u0627\u0628\u0644 \u0644\u0644\u0642\u0631\u0627\u0621\u0629\n",
        "generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "# \u0637\u0628\u0627\u0639\u0629 \u0627\u0644\u0646\u0635 \u0627\u0644\u0646\u0627\u062a\u062c\n",
        "print(generated_text)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "system\n",
            "You are a helpful assistant.\n",
            "user\n",
            "Hello!\n",
            "assistant\n",
            "Hello!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Solve step by step: A store sells apples for $2 each and oranges for $3 each. Tom bought 5 apples and 4 oranges. How much did he spend?\"\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "outputs = llm.generate([text], SamplingParams(temperature=0.2, max_tokens=5))\n",
        "print(outputs[0][\"text\"])"
      ],
      "metadata": {
        "id": "vE4kKeYHLDKo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}