# -*- coding: utf-8 -*-
"""suc_tencent_WeDLM-8B-Instruct_cpu.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19iqexStXZrXH0Ip0Z80la66xK3nZHiqQ
"""









!pip install psutil ninja packaging

!pip install -r 1.txt

# DO NOT use: pip install -r requirements.txt
# Use: bash install.sh
#
# This file pins versions for reference only.
# flash-attn requires torch first and --no-build-isolation flag.

--extra-index-url https://download.pytorch.org/whl/cu129
torch==2.8.0+cu129
transformers==4.56.1
triton==3.4.0
xxhash==3.6.0
numpy==2.3.4
tqdm==4.67.1
safetensors==0.6.2
huggingface_hub==0.35.0
flask==3.1.1

# flash-attn build dependencies
psutil
ninja
packaging

# flash-attn (install with: pip install flash-attn --no-build-isolation)
flash-attn==2.7.4.post1

!git clone https://github.com/Tencent/WeDLM.git

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/WeDLM

from transformers import AutoTokenizer
from wedlm import LLM, SamplingParams

llm = LLM(model="tencent/WeDLM-8B-Instruct")
tokenizer = AutoTokenizer.from_pretrained("tencent/WeDLM-8B-Instruct", trust_remote_code=True, torch_dtype="auto", device_map="auto")

prompt = "Solve step by step: A store sells apples for $2 each and oranges for $3 each. Tom bought 5 apples and 4 oranges. How much did he spend?"
messages = [{"role": "user", "content": prompt}]
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

outputs = llm.generate([text], SamplingParams(temperature=0.2, max_tokens=512))
print(outputs[0]["text"])

import os
if not os.path.exists('install.sh'):
  print('install.sh not found. Please ensure you are in the /content/WeDLM directory.')
else:
  !bash install.sh

import os
if not os.path.exists('install.sh'):
  print('install.sh not found. Please ensure you are in the /content/WeDLM directory.')
else:
  !bash install.sh

messages = [
    {"role": "user", "content": "What is the derivative of x^2?"},
    {"role": "assistant", "content": "The derivative of x² is 2x."},
    {"role": "user", "content": "What about x^3?"}
]
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
outputs = llm.generate([text], SamplingParams(temperature=0.2, max_tokens=256))

from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("tencent/WeDLM-8B-Instruct", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    "tencent/WeDLM-8B-Instruct",
    trust_remote_code=True,
    torch_dtype="auto",
    device_map="auto"
)

messages = [{"role": "user", "content": "Hello!"}]
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = tokenizer(text, return_tensors="pt").to(model.device)
outputs = model(**inputs)

from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("tencent/WeDLM-8B-Instruct", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    "tencent/WeDLM-8B-Instruct",
    trust_remote_code=True,
    torch_dtype="auto",
    device_map="auto"
)

messages = [{"role": "user", "content": "Hello!"}]
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = tokenizer(text, return_tensors="pt").to(model.device)
outputs = model(**inputs)

print(outputs)

# لتوليد رد فعلي من النموذج
output_tokens = model.generate(**inputs, max_new_tokens=50, do_sample=True, top_k=50, top_p=0.95, temperature=0.7)

# فك ترميز الرموز الناتجة إلى نص قابل للقراءة
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)

# طباعة النص الناتج
print(generated_text)

prompt = "Solve step by step: A store sells apples for $2 each and oranges for $3 each. Tom bought 5 apples and 4 oranges. How much did he spend?"
messages = [{"role": "user", "content": prompt}]
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

outputs = llm.generate([text], SamplingParams(temperature=0.2, max_tokens=5))
print(outputs[0]["text"])